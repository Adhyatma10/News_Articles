{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will provide you with all the analysis I have made regarding the dataset, after going through them carefully, I want you to come up with ML models and techniques we can use to predict the popularity.\n",
    "1. Topic is one of the four values {'obama','microsoft','palestine','economy'} The occurences of these Topic are as followed {28610,21858,8843,33928} in the same order as the topic list.\n",
    "2. Number of Articles Published by Day of the Week: \n",
    "DayOfWeek\n",
    "Monday       16250\n",
    "Tuesday      16720\n",
    "Wednesday    15739\n",
    "Thursday     15424\n",
    "Friday       13846\n",
    "Saturday      6994\n",
    "Sunday        82662. \n",
    "3. Number of Articles Published by Time of Day:\n",
    "TimeOfDay\n",
    "Morning (6 AM - 12 PM)      20257\n",
    "Afternoon (12 PM - 5 PM)    23551\n",
    "Evening (5 PM - 9 PM)       17374\n",
    "Night (9 PM - 6 AM)         32057\n",
    "4. Top Topics by Platform:\n",
    "Facebook: obama (Average Popularity = 2411.93)\n",
    "GooglePlus: palestine (Average Popularity = 2480.58)\n",
    "LinkedIn: microsoft (Average Popularity = 909.74)\n",
    "Insights: \n",
    "\n",
    "* *Obama and Palestine perform better on facebook as compared to microsoft and economy, suggesting sentimental topics have a higher popularity on Facebook as compared to other topics.*\n",
    "\n",
    "* *Global Topics like palestine and economy perform good on GooglePlus.*\n",
    "\n",
    "* *Microsoft and Economy perform the best in LinkedIn as they directly relate to job opportunities and purpose of the platform.*\n",
    "\n",
    "5. *Insight: Almost all articles with a repeating sources for a topic attains a lot of traction as compared to the other average occuring sources on all platforms irrespective of the topic. Therefore we can conclude Source is a highly important parameter for an articles popularity irrespective of the platfrom or topic. A repetative source for a topic will attain on an average higher popularity as compared to sources that are not frequent.*\n",
    "\n",
    "6. There are two more columns in the news_df Sentiment_Headline and Sentiment_Title\n",
    "I combined the two columns using average and created a temporary mean_sentiment column and then analysed popularity vs upper, lower quartile of mean_sentiment and got the below results:                                      \n",
    "                                          Facebook\tGooglePlus\tLinkedIn \n",
    "sentiment_category\t\t\t\n",
    "Extreme +vs Sentiment\t1890.259258\t1824.372085\t808.876082\n",
    "Extreme -ve Sentiment\t1745.458791\t1869.388414\t810.139478\n",
    "Medium Sentiment\t1600.414076\t1752.636002\t762.046227\n",
    "*Insight - Articles with extreme positive or negative Headline/Title perform better on all platforms as compared to modest sentiment Headline/Title article. The articles follow \"Bad publicity is Good publicity\". Articles with extreme negative values perform even better than positive articles on googleplus and linkedin.*\n",
    "\n",
    "7. *Insight- In general we observe that articles having the most common words in headline/title for a topic have higher average popularity than others, this could be because of \"Hot Topic\" phenomena, when there is an important news regarding any of the one topic most of the sources write about it and the Title/Headline being highly repeated gains a higher popularity than other not so relevant titles/headlines for a topic.* \n",
    "\n",
    "8. *Insight - we can say that articles published on weekends gain traction on facebook and googleplus while the opposite is true for linkedin which is more of a business/corporate application, mostly used during week days.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Cleaned_News.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IDLink', 'Title', 'Headline', 'Source', 'Topic', 'PublishDate',\n",
       "       'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus',\n",
       "       'LinkedIn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PublishDate'] = pd.to_datetime(data['PublishDate'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new Columns \n",
    "import pandas as pd\n",
    "\n",
    "def create_derived_features(df):\n",
    "    \"\"\"Creates derived features from existing columns.\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame containing news data with 'SentimentTitle',\n",
    "            'SentimentHeadline', and 'PublishDate' columns.\n",
    "\n",
    "    Returns:\n",
    "        A new Pandas DataFrame with the derived features, or the original\n",
    "        DataFrame if the required columns are missing. Returns None if input is not a dataframe.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Input is not a Pandas DataFrame\")\n",
    "        return None\n",
    "    df_derived = df.copy()\n",
    "\n",
    "    required_cols = ['SentimentTitle', 'SentimentHeadline', 'PublishDate']\n",
    "    if not all(col in df_derived.columns for col in required_cols):\n",
    "        print(f\"Missing required columns: {set(required_cols) - set(df_derived.columns)}\")\n",
    "        return df_derived  # Return original if columns are missing\n",
    "\n",
    "    # 1. Sentiment Mean\n",
    "    df_derived['Sentiment_mean'] = df_derived[['SentimentTitle', 'SentimentHeadline']].mean(axis=1)\n",
    "\n",
    "    # 2. Publish Day\n",
    "    df_derived['PublishDay'] = df_derived['PublishDate'].dt.day_name()\n",
    "\n",
    "    # 3. Publish Hour\n",
    "    df_derived['PublishHour'] = df_derived['PublishDate'].dt.hour\n",
    "\n",
    "    # 4. Time of Day\n",
    "    def categorize_time(hour):\n",
    "        if 6 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'Afternoon'\n",
    "        elif 17 <= hour < 21:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "\n",
    "    df_derived['TimeOfDay'] = df_derived['PublishHour'].apply(categorize_time)\n",
    "\n",
    "    return df_derived\n",
    "\n",
    "data = create_derived_features(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Sentiment_mean</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>PublishHour</th>\n",
       "      <th>TimeOfDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99248</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.053300</td>\n",
       "      <td>2547.659722</td>\n",
       "      <td>1538.570833</td>\n",
       "      <td>499.025000</td>\n",
       "      <td>-0.026650</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10423</td>\n",
       "      <td>A Look at the Health of the Chinese Economy</td>\n",
       "      <td>Tim Haywood investment director businessunit h...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2008-09-20 00:00:00</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>-0.156386</td>\n",
       "      <td>1380.145833</td>\n",
       "      <td>1957.444444</td>\n",
       "      <td>753.729167</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18828</td>\n",
       "      <td>Nouriel Roubini Global Economy Not Back to 2008</td>\n",
       "      <td>Nouriel Roubini NYU professor and chairman at ...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "      <td>-0.425210</td>\n",
       "      <td>0.139754</td>\n",
       "      <td>1647.295833</td>\n",
       "      <td>2242.472222</td>\n",
       "      <td>874.993056</td>\n",
       "      <td>-0.142728</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27788</td>\n",
       "      <td>Finland GDP Expands In Q4</td>\n",
       "      <td>Finlands economy expanded marginally in the th...</td>\n",
       "      <td>RTT News</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:06:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>1157.554167</td>\n",
       "      <td>1805.383333</td>\n",
       "      <td>701.736111</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27789</td>\n",
       "      <td>Tourism govt spending buoys Thai economy in Ja...</td>\n",
       "      <td>Tourism and public spending continued to boost...</td>\n",
       "      <td>The Nation Thailand39s English news</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:11:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141084</td>\n",
       "      <td>1439.512500</td>\n",
       "      <td>2166.450000</td>\n",
       "      <td>857.687500</td>\n",
       "      <td>0.070542</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDLink                                              Title  \\\n",
       "0   99248   Obama Lays Wreath at Arlington National Cemetery   \n",
       "1   10423        A Look at the Health of the Chinese Economy   \n",
       "2   18828    Nouriel Roubini Global Economy Not Back to 2008   \n",
       "3   27788                          Finland GDP Expands In Q4   \n",
       "4   27789  Tourism govt spending buoys Thai economy in Ja...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...   \n",
       "1  Tim Haywood investment director businessunit h...   \n",
       "2  Nouriel Roubini NYU professor and chairman at ...   \n",
       "3  Finlands economy expanded marginally in the th...   \n",
       "4  Tourism and public spending continued to boost...   \n",
       "\n",
       "                                Source    Topic         PublishDate  \\\n",
       "0                            USA TODAY    obama 2002-04-02 00:00:00   \n",
       "1                            Bloomberg  economy 2008-09-20 00:00:00   \n",
       "2                            Bloomberg  economy 2012-01-28 00:00:00   \n",
       "3                             RTT News  economy 2015-03-01 00:06:00   \n",
       "4  The Nation Thailand39s English news  economy 2015-03-01 00:11:00   \n",
       "\n",
       "   SentimentTitle  SentimentHeadline     Facebook   GooglePlus    LinkedIn  \\\n",
       "0        0.000000          -0.053300  2547.659722  1538.570833  499.025000   \n",
       "1        0.208333          -0.156386  1380.145833  1957.444444  753.729167   \n",
       "2       -0.425210           0.139754  1647.295833  2242.472222  874.993056   \n",
       "3        0.000000           0.026064  1157.554167  1805.383333  701.736111   \n",
       "4        0.000000           0.141084  1439.512500  2166.450000  857.687500   \n",
       "\n",
       "   Sentiment_mean PublishDay  PublishHour TimeOfDay  \n",
       "0       -0.026650    Tuesday            0     Night  \n",
       "1        0.025974   Saturday            0     Night  \n",
       "2       -0.142728   Saturday            0     Night  \n",
       "3        0.013032     Sunday            0     Night  \n",
       "4        0.070542     Sunday            0     Night  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocesses the data for social media engagement prediction.\"\"\"\n",
    "\n",
    "\n",
    "    df['Facebook'] = np.log1p(df['Facebook'] + 1)\n",
    "    df['GooglePlus'] = np.log1p(df['GooglePlus'] + 1)\n",
    "    df['LinkedIn'] = np.log1p(df['LinkedIn'] + 1)\n",
    "\n",
    "    source_counts = df.groupby(['Topic', 'Source'])['Source'].count().unstack(fill_value=0)\n",
    "    top_10_sources = {}\n",
    "    for topic in df['Topic'].unique():\n",
    "        top_10_sources[topic] = source_counts.loc[topic].nlargest(10).index.tolist()\n",
    "\n",
    "    for topic, sources in top_10_sources.items():\n",
    "        for source in sources:\n",
    "            source_col = f\"source_is_{topic}_{source.replace(' ', '_')}\"\n",
    "            df[source_col] = (df['Topic'] == topic) & (df['Source'] == source)\n",
    "            \n",
    "    categorical_features = ['Topic', 'PublishDay', 'TimeOfDay']\n",
    "    for feature in categorical_features:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        encoded = ohe.fit_transform(df[[feature]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out([feature]))\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df['Sentiment_mean_scaled'] = scaler.fit_transform(df[['Sentiment_mean']].values)\n",
    "\n",
    "    df['is_weekend'] = (df['PublishDay_Saturday'] == 1) | (df['PublishDay_Sunday'] == 1)\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"  # Return empty string instead of list for CountVectorizer\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = [word for word in text.split() if word not in stop_words]\n",
    "        return \" \".join(tokens)  # Return space-separated string\n",
    "\n",
    "    df['CleanedHeadline'] = df['Headline'].apply(preprocess_text)\n",
    "    df['CleanedTitle'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "    headline_vectorizer = CountVectorizer(max_features=20, ngram_range=(5, 5))\n",
    "    headline_patterns = headline_vectorizer.fit_transform(df['CleanedHeadline']).toarray()\n",
    "    df = pd.concat([df, pd.DataFrame(headline_patterns, columns=headline_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "    title_vectorizer = CountVectorizer(max_features=20, ngram_range=(3, 3))\n",
    "    title_patterns = title_vectorizer.fit_transform(df['CleanedTitle']).toarray()\n",
    "    df = pd.concat([df, pd.DataFrame(title_patterns, columns=title_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "    feature_cols = list(df.filter(like='is_')) + list(df.filter(like='PublishDay_')) + list(df.filter(like='TimeOfDay_')) + ['Sentiment_mean_scaled'] + list(df.filter(like='source_is_')) + list(headline_vectorizer.get_feature_names_out()) + list(title_vectorizer.get_feature_names_out()) + ['is_weekend']\n",
    "    X = df[feature_cols]\n",
    "    X = X.loc[:, (X != X.iloc[0]).any()]\n",
    "\n",
    "    return X, df[['Facebook', 'GooglePlus', 'LinkedIn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Facebook_log'] = np.log1p(df['Facebook'] + 1)\n",
    "df['GooglePlus_log'] = np.log1p(df['GooglePlus'] + 1)\n",
    "df['LinkedIn_log'] = np.log1p(df['LinkedIn'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date-time components\n",
    "df['Year'] = pd.to_datetime(df['PublishDate']).dt.year\n",
    "df['Month'] = pd.to_datetime(df['PublishDate']).dt.month\n",
    "df['Day'] = pd.to_datetime(df['PublishDate']).dt.day\n",
    "df['Hour'] = pd.to_datetime(df['PublishDate']).dt.hour\n",
    "\n",
    "# Cyclical encoding for Month and Day\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute topic-wise source frequency\n",
    "source_topic_counts = df.groupby(['Topic', 'Source']).size().reset_index(name='SourceCount')\n",
    "total_topic_counts = df.groupby('Topic').size().reset_index(name='TotalCount')\n",
    "\n",
    "# Merge to calculate normalized frequency\n",
    "df = df.merge(source_topic_counts, on=['Topic', 'Source'], how='left')\n",
    "df = df.merge(total_topic_counts, on='Topic', how='left')\n",
    "df['SourceFreq'] = df['SourceCount'] / df['TotalCount']\n",
    "df.drop(['SourceCount', 'TotalCount'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "# Helper function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and convert to lowercase\n",
    "    stop_words = set([\"the\", \"and\", \"is\", \"to\", \"in\", \"it\", \"of\", \"for\", \"on\", \"with\", \"as\", \"this\", \"at\", \"by\"])  # Example stop words\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['CleanedHeadline'] = df['Headline'].apply(preprocess_text)\n",
    "df['CleanedTitle'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "# Get top 20 patterns for headline (5 words)\n",
    "headline_vectorizer = CountVectorizer(max_features=20, ngram_range=(5, 5))\n",
    "headline_patterns = headline_vectorizer.fit(df['CleanedHeadline']).get_feature_names_out()\n",
    "\n",
    "# Get top 20 patterns for title (3 words)\n",
    "title_vectorizer = CountVectorizer(max_features=20, ngram_range=(3, 3))\n",
    "title_patterns = title_vectorizer.fit(df['CleanedTitle']).get_feature_names_out()\n",
    "\n",
    "# Add binary columns for presence of these patterns\n",
    "for pattern in headline_patterns:\n",
    "    df[f'Headline_{pattern}'] = df['CleanedHeadline'].str.contains(pattern).astype(int)\n",
    "\n",
    "for pattern in title_patterns:\n",
    "    df[f'Title_{pattern}'] = df['CleanedTitle'].str.contains(pattern).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Targets\n",
    "features = [\n",
    "    'Topic', 'PublishHour', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', \n",
    "    'SourceFreq', 'Sentiment_mean'] + \\\n",
    "    [f'Headline_{pattern}' for pattern in headline_patterns] + \\\n",
    "    [f'Title_{pattern}' for pattern in title_patterns]\n",
    "\n",
    "target_facebook = 'Facebook_log'\n",
    "target_googleplus = 'GooglePlus_log'\n",
    "target_linkedin = 'LinkedIn_log'\n",
    "\n",
    "X = df[features]\n",
    "y_facebook = df[target_facebook]\n",
    "y_googleplus = df[target_googleplus]\n",
    "y_linkedin = df[target_linkedin]\n",
    "\n",
    "\n",
    "\n",
    "# Train-test split for each target\n",
    "X_train_fb, X_test_fb, y_train_fb, y_test_fb = train_test_split(X, y_facebook, test_size=0.2, random_state=42)\n",
    "X_train_gp, X_test_gp, y_train_gp, y_test_gp = train_test_split(X, y_googleplus, test_size=0.2, random_state=42)\n",
    "X_train_li, X_test_li, y_train_li, y_test_li = train_test_split(X, y_linkedin, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic                                                     float64\n",
      "PublishHour                                                 int32\n",
      "Month_sin                                                 float64\n",
      "Month_cos                                                 float64\n",
      "Day_sin                                                   float64\n",
      "Day_cos                                                   float64\n",
      "SourceFreq                                                float64\n",
      "Sentiment_mean                                            float64\n",
      "Headline_canadian prime minister justin trudeau             int32\n",
      "Headline_external affairs minister sushma swaraj            int32\n",
      "Headline_federal reserve chair janet yellen                 int32\n",
      "Headline_his final state union address                      int32\n",
      "Headline_international middle east media center             int32\n",
      "Headline_japanese prime minister shinzo abe                 int32\n",
      "Headline_middle east media center wwwimemcorg               int32\n",
      "Headline_obama first lady michelle obama                    int32\n",
      "Headline_palestine today service international middle       int32\n",
      "Headline_president barack obama said thursday               int32\n",
      "Headline_president barack obama will meet                   int32\n",
      "Headline_service international middle east media            int32\n",
      "Headline_today service international middle east            int32\n",
      "Headline_us president barack obama has                      int32\n",
      "Headline_us president barack obama said                     int32\n",
      "Headline_us president barack obama will                     int32\n",
      "Headline_washington ap president barack obama               int32\n",
      "Headline_washington cnn president barack obama              int32\n",
      "Headline_washington us president barack obama               int32\n",
      "Headline_welcome palestine today service international      int32\n",
      "Title_ceo satya nadella                                     int32\n",
      "Title_final state union                                     int32\n",
      "Title_lumia 950 xl                                          int32\n",
      "Title_market economy status                                 int32\n",
      "Title_microsoft ceo satya                                   int32\n",
      "Title_microsoft co msft                                     int32\n",
      "Title_microsoft corporation msft                            int32\n",
      "Title_microsoft lumia 650                                   int32\n",
      "Title_microsoft lumia 950                                   int32\n",
      "Title_microsoft office 365                                  int32\n",
      "Title_microsoft surface book                                int32\n",
      "Title_microsoft surface pro                                 int32\n",
      "Title_new windows 10                                        int32\n",
      "Title_obama says us                                         int32\n",
      "Title_president barack obama                                int32\n",
      "Title_supreme court nominee                                 int32\n",
      "Title_us economy grew                                       int32\n",
      "Title_windows 10 mobile                                     int32\n",
      "Title_windows 10 update                                     int32\n",
      "Title_windows 10 upgrade                                    int32\n",
      "dtype: object\n",
      "Topic                                                     float64\n",
      "PublishHour                                                 int32\n",
      "Month_sin                                                 float64\n",
      "Month_cos                                                 float64\n",
      "Day_sin                                                   float64\n",
      "Day_cos                                                   float64\n",
      "SourceFreq                                                float64\n",
      "Sentiment_mean                                            float64\n",
      "Headline_canadian prime minister justin trudeau             int32\n",
      "Headline_external affairs minister sushma swaraj            int32\n",
      "Headline_federal reserve chair janet yellen                 int32\n",
      "Headline_his final state union address                      int32\n",
      "Headline_international middle east media center             int32\n",
      "Headline_japanese prime minister shinzo abe                 int32\n",
      "Headline_middle east media center wwwimemcorg               int32\n",
      "Headline_obama first lady michelle obama                    int32\n",
      "Headline_palestine today service international middle       int32\n",
      "Headline_president barack obama said thursday               int32\n",
      "Headline_president barack obama will meet                   int32\n",
      "Headline_service international middle east media            int32\n",
      "Headline_today service international middle east            int32\n",
      "Headline_us president barack obama has                      int32\n",
      "Headline_us president barack obama said                     int32\n",
      "Headline_us president barack obama will                     int32\n",
      "Headline_washington ap president barack obama               int32\n",
      "Headline_washington cnn president barack obama              int32\n",
      "Headline_washington us president barack obama               int32\n",
      "Headline_welcome palestine today service international      int32\n",
      "Title_ceo satya nadella                                     int32\n",
      "Title_final state union                                     int32\n",
      "Title_lumia 950 xl                                          int32\n",
      "Title_market economy status                                 int32\n",
      "Title_microsoft ceo satya                                   int32\n",
      "Title_microsoft co msft                                     int32\n",
      "Title_microsoft corporation msft                            int32\n",
      "Title_microsoft lumia 650                                   int32\n",
      "Title_microsoft lumia 950                                   int32\n",
      "Title_microsoft office 365                                  int32\n",
      "Title_microsoft surface book                                int32\n",
      "Title_microsoft surface pro                                 int32\n",
      "Title_new windows 10                                        int32\n",
      "Title_obama says us                                         int32\n",
      "Title_president barack obama                                int32\n",
      "Title_supreme court nominee                                 int32\n",
      "Title_us economy grew                                       int32\n",
      "Title_windows 10 mobile                                     int32\n",
      "Title_windows 10 update                                     int32\n",
      "Title_windows 10 upgrade                                    int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ensure all columns are numeric\n",
    "X_train_fb = X_train_fb.apply(pd.to_numeric, errors='coerce')\n",
    "X_test_fb = X_test_fb.apply(pd.to_numeric, errors='coerce')\n",
    "X_train_gp = X_train_gp.apply(pd.to_numeric, errors='coerce')\n",
    "X_test_gp = X_test_gp.apply(pd.to_numeric, errors='coerce')\n",
    "X_train_li = X_train_li.apply(pd.to_numeric, errors='coerce')\n",
    "X_test_li = X_test_li.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check for any remaining non-numeric columns\n",
    "print(X_train_fb.dtypes)\n",
    "print(X_test_fb.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 665\n",
      "[LightGBM] [Info] Number of data points in the train set: 74591, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 7.387639\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 665\n",
      "[LightGBM] [Info] Number of data points in the train set: 74591, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 7.460173\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 665\n",
      "[LightGBM] [Info] Number of data points in the train set: 74591, number of used features: 44\n",
      "[LightGBM] [Info] Start training from score 6.625275\n",
      "RMSE for Facebook: 0.20451280810356615\n",
      "RMSE for GooglePlus: 0.1467251658915226\n",
      "RMSE for LinkedIn: 0.18870089063629905\n"
     ]
    }
   ],
   "source": [
    "# Train LightGBM Models\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def train_lgbm(X_train, y_train, X_test, y_test):\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    return model, rmse\n",
    "\n",
    "# Train and evaluate for each target\n",
    "model_fb, rmse_fb = train_lgbm(X_train_fb, y_train_fb, X_test_fb, y_test_fb)\n",
    "model_gp, rmse_gp = train_lgbm(X_train_gp, y_train_gp, X_test_gp, y_test_gp)\n",
    "model_li, rmse_li = train_lgbm(X_train_li, y_train_li, X_test_li, y_test_li)\n",
    "\n",
    "# Print results\n",
    "print(f\"RMSE for Facebook: {rmse_fb}\")\n",
    "print(f\"RMSE for GooglePlus: {rmse_gp}\")\n",
    "print(f\"RMSE for LinkedIn: {rmse_li}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for model_fb: 0.20451280810356615\n",
      "RMSE for model_gp: 0.1467251658915226\n",
      "RMSE for model_li: 0.18870089063629905\n",
      "Test RMSE for model_fb: 0.20451280810356615\n",
      "Test RMSE for model_gp: 0.1467251658915226\n",
      "Test RMSE for model_li: 0.18870089063629905\n"
     ]
    }
   ],
   "source": [
    "# Test the models and compare RMSE values\n",
    "print(f\"RMSE for model_fb: {rmse_fb}\")\n",
    "print(f\"RMSE for model_gp: {rmse_gp}\")\n",
    "print(f\"RMSE for model_li: {rmse_li}\")\n",
    "\n",
    "# Make predictions with each model\n",
    "predictions_fb = model_fb.predict(X_test_fb)\n",
    "predictions_gp = model_gp.predict(X_test_gp)\n",
    "predictions_li = model_li.predict(X_test_li)\n",
    "\n",
    "# Optionally: Calculate RMSE for each model's predictions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "rmse_fb_test = np.sqrt(mean_squared_error(y_test_fb, predictions_fb))\n",
    "rmse_gp_test = np.sqrt(mean_squared_error(y_test_gp, predictions_gp))\n",
    "rmse_li_test = np.sqrt(mean_squared_error(y_test_li, predictions_li))\n",
    "\n",
    "print(f\"Test RMSE for model_fb: {rmse_fb_test}\")\n",
    "print(f\"Test RMSE for model_gp: {rmse_gp_test}\")\n",
    "print(f\"Test RMSE for model_li: {rmse_li_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Scale RMSE for model_fb: 363.19203719791534\n",
      "Original Scale RMSE for model_gp: 274.7420048319023\n",
      "Original Scale RMSE for model_li: 138.58863271095132\n"
     ]
    }
   ],
   "source": [
    "# Inverse log1p transformation for predictions\n",
    "predictions_fb_original = np.expm1(predictions_fb)\n",
    "predictions_gp_original = np.expm1(predictions_gp)\n",
    "predictions_li_original = np.expm1(predictions_li)\n",
    "\n",
    "# Inverse log1p transformation for actual values\n",
    "y_test_fb_original = np.expm1(y_test_fb)\n",
    "y_test_gp_original = np.expm1(y_test_gp)\n",
    "y_test_li_original = np.expm1(y_test_li)\n",
    "\n",
    "# Calculate RMSE in the original scale\n",
    "rmse_fb_original = np.sqrt(mean_squared_error(y_test_fb_original, predictions_fb_original))\n",
    "rmse_gp_original = np.sqrt(mean_squared_error(y_test_gp_original, predictions_gp_original))\n",
    "rmse_li_original = np.sqrt(mean_squared_error(y_test_li_original, predictions_li_original))\n",
    "\n",
    "print(f\"Original Scale RMSE for model_fb: {rmse_fb_original}\")\n",
    "print(f\"Original Scale RMSE for model_gp: {rmse_gp_original}\")\n",
    "print(f\"Original Scale RMSE for model_li: {rmse_li_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facebook</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>GooglePlus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93239.000000</td>\n",
       "      <td>93239.000000</td>\n",
       "      <td>93239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1709.134605</td>\n",
       "      <td>785.776226</td>\n",
       "      <td>1799.756127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>600.936341</td>\n",
       "      <td>228.102370</td>\n",
       "      <td>505.231734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>482.225000</td>\n",
       "      <td>134.412500</td>\n",
       "      <td>441.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1228.854167</td>\n",
       "      <td>579.208333</td>\n",
       "      <td>1422.045833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1540.062500</td>\n",
       "      <td>801.472222</td>\n",
       "      <td>1695.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2115.179167</td>\n",
       "      <td>928.319444</td>\n",
       "      <td>2077.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5763.013889</td>\n",
       "      <td>1975.381944</td>\n",
       "      <td>4503.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Facebook      LinkedIn    GooglePlus\n",
       "count  93239.000000  93239.000000  93239.000000\n",
       "mean    1709.134605    785.776226   1799.756127\n",
       "std      600.936341    228.102370    505.231734\n",
       "min      482.225000    134.412500    441.187500\n",
       "25%     1228.854167    579.208333   1422.045833\n",
       "50%     1540.062500    801.472222   1695.366667\n",
       "75%     2115.179167    928.319444   2077.100000\n",
       "max     5763.013889   1975.381944   4503.333333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Facebook\",\"LinkedIn\",\"GooglePlus\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FB Test RMSE =  363.32\n",
    "\n",
    "GP Test RMSE = 274.68\n",
    "\n",
    "LI Test RMSE = 138.716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Facebook:\n",
      "  MSE: 263899.43\n",
      "  RMSE: 513.71\n",
      "  R-squared: 0.27\n",
      "------------------------------\n",
      "Results for GooglePlus:\n",
      "  MSE: 192197.84\n",
      "  RMSE: 438.40\n",
      "  R-squared: 0.24\n",
      "------------------------------\n",
      "Results for LinkedIn:\n",
      "  MSE: 34805.38\n",
      "  RMSE: 186.56\n",
      "  R-squared: 0.35\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "\n",
    "news=data.copy()\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#news=pd.read_csv(\"Tableau_News.csv\")\n",
    "# Preprocessing and Feature Engineering\n",
    "\n",
    "# Log transform the target variables\n",
    "news['Facebook'] = np.log1p(news['Facebook'])\n",
    "news['GooglePlus'] = np.log1p(news['GooglePlus'])\n",
    "news['LinkedIn'] = np.log1p(news['LinkedIn'])\n",
    "\n",
    "# One-Hot Encoding for Topic\n",
    "ohe_topic = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "topic_encoded = ohe_topic.fit_transform(news[['Topic']])\n",
    "topic_df = pd.DataFrame(topic_encoded, columns=ohe_topic.get_feature_names_out(['Topic']))\n",
    "news = pd.concat([news, topic_df], axis=1)\n",
    "\n",
    "# One-Hot Encoding for Day of the Week\n",
    "ohe_day = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "day_encoded = ohe_day.fit_transform(news[['PublishDay']])\n",
    "day_df = pd.DataFrame(day_encoded, columns=ohe_day.get_feature_names_out(['PublishDay']))\n",
    "news = pd.concat([news, day_df], axis=1)\n",
    "\n",
    "# One-Hot Encoding for Time of Day\n",
    "ohe_time = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "time_encoded = ohe_time.fit_transform(news[['TimeOfDay']])\n",
    "time_df = pd.DataFrame(time_encoded, columns=ohe_time.get_feature_names_out(['TimeOfDay']))\n",
    "news = pd.concat([news, time_df], axis=1)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = StandardScaler()\n",
    "news['Sentiment_mean_scaled'] = scaler.fit_transform(news[['Sentiment_mean']].values)\n",
    "\n",
    "# Feature Engineering: Source (Frequency Encoding)\n",
    "source_counts = news.groupby(['Topic', 'Source'])['Source'].count().unstack(fill_value=0)\n",
    "top_10_sources = {}\n",
    "for topic in news['Topic'].unique():\n",
    "    top_10_sources[topic] = source_counts.loc[topic].nlargest(10).index.tolist()\n",
    "\n",
    "for topic, sources in top_10_sources.items():\n",
    "    for source in sources:\n",
    "        source_col = f\"source_is_{topic}_{source.replace(' ', '_')}\"  # more robust column names\n",
    "        news[source_col] = (news['Topic'] == topic) & (news['Source'] == source)\n",
    "\n",
    "# Feature Engineering: Interaction Terms\n",
    "news['is_weekend'] = (news['PublishDay'] == \"Sunday\") | (news['PublishDay'] == \"Saturday\")\n",
    "\n",
    "# Text Preprocessing (for Headline and Title)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float) and math.isnan(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "news['CleanedHeadline'] = news['Headline'].apply(preprocess_text)\n",
    "news['CleanedTitle'] = news['Title'].apply(preprocess_text)\n",
    "\n",
    "# Feature Engineering: Common Words in Headline and Title (using CountVectorizer)\n",
    "headline_vectorizer = CountVectorizer(max_features=20, ngram_range=(5, 5))\n",
    "headline_patterns = headline_vectorizer.fit_transform(news['CleanedHeadline'].apply(lambda x: \" \".join(x))).toarray()\n",
    "news = pd.concat([news, pd.DataFrame(headline_patterns, columns=headline_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "title_vectorizer = CountVectorizer(max_features=20, ngram_range=(3, 3))\n",
    "title_patterns = title_vectorizer.fit_transform(news['CleanedTitle'].apply(lambda x: \" \".join(x))).toarray()\n",
    "news = pd.concat([news, pd.DataFrame(title_patterns, columns=title_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "# Define features (X) and targets (y)\n",
    "feature_cols = list(news.filter(like='is_')) + list(news.filter(like='PublishDay_')) + list(news.filter(like='TimeOfDay_')) + ['Sentiment_mean_scaled'] + list(news.filter(like='source_is_')) + list(headline_vectorizer.get_feature_names_out()) + list(title_vectorizer.get_feature_names_out()) + ['is_weekend']\n",
    "X = news[feature_cols]\n",
    "\n",
    "targets = ['Facebook', 'GooglePlus', 'LinkedIn']\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    y = news[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Linear Regression using scikit-learn\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Reverse the log transform\n",
    "    y_test_original = np.expm1(y_test)\n",
    "    y_pred_original = np.expm1(y_pred)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "    results[target] = {'MSE': mse, 'RMSE': rmse, 'R-squared': r2}\n",
    "\n",
    "# Print results\n",
    "for target, metrics in results.items():\n",
    "    print(f\"Results for {target}:\")\n",
    "    print(f\"  MSE: {metrics['MSE']:.2f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f}\")\n",
    "    print(f\"  R-squared: {metrics['R-squared']:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IDLink', 'Title', 'Headline', 'Source', 'Topic', 'PublishDate',\n",
       "       'SentimentTitle', 'SentimentHeadline', 'Facebook', 'GooglePlus',\n",
       "       'LinkedIn', 'Sentiment_mean', 'PublishDay', 'PublishHour', 'TimeOfDay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDLink</th>\n",
       "      <th>Title</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PublishDate</th>\n",
       "      <th>SentimentTitle</th>\n",
       "      <th>SentimentHeadline</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>GooglePlus</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Sentiment_mean</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>PublishHour</th>\n",
       "      <th>TimeOfDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99248</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemetery</td>\n",
       "      <td>Obama Lays Wreath at Arlington National Cemete...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>obama</td>\n",
       "      <td>2002-04-02 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.053300</td>\n",
       "      <td>2547.659722</td>\n",
       "      <td>1538.570833</td>\n",
       "      <td>499.025000</td>\n",
       "      <td>-0.026650</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10423</td>\n",
       "      <td>A Look at the Health of the Chinese Economy</td>\n",
       "      <td>Tim Haywood investment director businessunit h...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2008-09-20 00:00:00</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>-0.156386</td>\n",
       "      <td>1380.145833</td>\n",
       "      <td>1957.444444</td>\n",
       "      <td>753.729167</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18828</td>\n",
       "      <td>Nouriel Roubini Global Economy Not Back to 2008</td>\n",
       "      <td>Nouriel Roubini NYU professor and chairman at ...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>economy</td>\n",
       "      <td>2012-01-28 00:00:00</td>\n",
       "      <td>-0.425210</td>\n",
       "      <td>0.139754</td>\n",
       "      <td>1647.295833</td>\n",
       "      <td>2242.472222</td>\n",
       "      <td>874.993056</td>\n",
       "      <td>-0.142728</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27788</td>\n",
       "      <td>Finland GDP Expands In Q4</td>\n",
       "      <td>Finlands economy expanded marginally in the th...</td>\n",
       "      <td>RTT News</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:06:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026064</td>\n",
       "      <td>1157.554167</td>\n",
       "      <td>1805.383333</td>\n",
       "      <td>701.736111</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27789</td>\n",
       "      <td>Tourism govt spending buoys Thai economy in Ja...</td>\n",
       "      <td>Tourism and public spending continued to boost...</td>\n",
       "      <td>The Nation Thailand39s English news</td>\n",
       "      <td>economy</td>\n",
       "      <td>2015-03-01 00:11:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141084</td>\n",
       "      <td>1439.512500</td>\n",
       "      <td>2166.450000</td>\n",
       "      <td>857.687500</td>\n",
       "      <td>0.070542</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDLink                                              Title  \\\n",
       "0   99248   Obama Lays Wreath at Arlington National Cemetery   \n",
       "1   10423        A Look at the Health of the Chinese Economy   \n",
       "2   18828    Nouriel Roubini Global Economy Not Back to 2008   \n",
       "3   27788                          Finland GDP Expands In Q4   \n",
       "4   27789  Tourism govt spending buoys Thai economy in Ja...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Obama Lays Wreath at Arlington National Cemete...   \n",
       "1  Tim Haywood investment director businessunit h...   \n",
       "2  Nouriel Roubini NYU professor and chairman at ...   \n",
       "3  Finlands economy expanded marginally in the th...   \n",
       "4  Tourism and public spending continued to boost...   \n",
       "\n",
       "                                Source    Topic         PublishDate  \\\n",
       "0                            USA TODAY    obama 2002-04-02 00:00:00   \n",
       "1                            Bloomberg  economy 2008-09-20 00:00:00   \n",
       "2                            Bloomberg  economy 2012-01-28 00:00:00   \n",
       "3                             RTT News  economy 2015-03-01 00:06:00   \n",
       "4  The Nation Thailand39s English news  economy 2015-03-01 00:11:00   \n",
       "\n",
       "   SentimentTitle  SentimentHeadline     Facebook   GooglePlus    LinkedIn  \\\n",
       "0        0.000000          -0.053300  2547.659722  1538.570833  499.025000   \n",
       "1        0.208333          -0.156386  1380.145833  1957.444444  753.729167   \n",
       "2       -0.425210           0.139754  1647.295833  2242.472222  874.993056   \n",
       "3        0.000000           0.026064  1157.554167  1805.383333  701.736111   \n",
       "4        0.000000           0.141084  1439.512500  2166.450000  857.687500   \n",
       "\n",
       "   Sentiment_mean PublishDay  PublishHour TimeOfDay  \n",
       "0       -0.026650    Tuesday            0     Night  \n",
       "1        0.025974   Saturday            0     Night  \n",
       "2       -0.142728   Saturday            0     Night  \n",
       "3        0.013032     Sunday            0     Night  \n",
       "4        0.070542     Sunday            0     Night  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved RMSE for Facebook: 0.3161 with n_estimators=10\n",
      "Improved RMSE for Facebook: 0.3117 with n_estimators=20\n",
      "Improved RMSE for Facebook: 0.3107 with n_estimators=30\n",
      "Improved RMSE for Facebook: 0.3098 with n_estimators=40\n",
      "Improved RMSE for Facebook: 0.3094 with n_estimators=50\n",
      "Improved RMSE for Facebook: 0.3092 with n_estimators=60\n",
      "Improved RMSE for Facebook: 0.3090 with n_estimators=70\n",
      "Improved RMSE for Facebook: 0.3089 with n_estimators=80\n",
      "Improved RMSE for Facebook: 0.3087 with n_estimators=90\n",
      "Improved RMSE for Facebook: 0.3085 with n_estimators=100\n",
      "Improved RMSE for GooglePlus: 0.2548 with n_estimators=10\n",
      "Improved RMSE for GooglePlus: 0.2515 with n_estimators=20\n",
      "Improved RMSE for GooglePlus: 0.2505 with n_estimators=30\n",
      "Improved RMSE for GooglePlus: 0.2499 with n_estimators=40\n",
      "Improved RMSE for GooglePlus: 0.2498 with n_estimators=50\n",
      "Improved RMSE for GooglePlus: 0.2495 with n_estimators=60\n",
      "Improved RMSE for GooglePlus: 0.2494 with n_estimators=70\n",
      "Improved RMSE for GooglePlus: 0.2493 with n_estimators=80\n",
      "Improved RMSE for GooglePlus: 0.2491 with n_estimators=90\n",
      "Improved RMSE for GooglePlus: 0.2490 with n_estimators=100\n",
      "Improved RMSE for LinkedIn: 0.2843 with n_estimators=10\n",
      "Improved RMSE for LinkedIn: 0.2811 with n_estimators=20\n",
      "Improved RMSE for LinkedIn: 0.2800 with n_estimators=30\n",
      "Improved RMSE for LinkedIn: 0.2790 with n_estimators=40\n",
      "Improved RMSE for LinkedIn: 0.2786 with n_estimators=50\n",
      "Improved RMSE for LinkedIn: 0.2785 with n_estimators=60\n",
      "Improved RMSE for LinkedIn: 0.2785 with n_estimators=70\n",
      "Improved RMSE for LinkedIn: 0.2784 with n_estimators=80\n",
      "Improved RMSE for LinkedIn: 0.2782 with n_estimators=90\n",
      "Improved RMSE for LinkedIn: 0.2781 with n_estimators=100\n",
      "Results for Facebook (Random Forest):\n",
      "  MSE: 282531.20\n",
      "  RMSE: 531.54\n",
      "  R-squared: 0.22\n",
      "------------------------------\n",
      "Results for GooglePlus (Random Forest):\n",
      "  MSE: 218617.72\n",
      "  RMSE: 467.57\n",
      "  R-squared: 0.14\n",
      "------------------------------\n",
      "Results for LinkedIn (Random Forest):\n",
      "  MSE: 42194.76\n",
      "  RMSE: 205.41\n",
      "  R-squared: 0.21\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocesses the data for social media engagement prediction.\"\"\"\n",
    "\n",
    "\n",
    "    df['Facebook'] = np.log1p(df['Facebook'] + 1)\n",
    "    df['GooglePlus'] = np.log1p(df['GooglePlus'] + 1)\n",
    "    df['LinkedIn'] = np.log1p(df['LinkedIn'] + 1)\n",
    "\n",
    "    source_counts = df.groupby(['Topic', 'Source'])['Source'].count().unstack(fill_value=0)\n",
    "    top_10_sources = {}\n",
    "    for topic in df['Topic'].unique():\n",
    "        top_10_sources[topic] = source_counts.loc[topic].nlargest(10).index.tolist()\n",
    "\n",
    "    for topic, sources in top_10_sources.items():\n",
    "        for source in sources:\n",
    "            source_col = f\"source_is_{topic}_{source.replace(' ', '_')}\"\n",
    "            df[source_col] = (df['Topic'] == topic) & (df['Source'] == source)\n",
    "            \n",
    "    categorical_features = ['Topic', 'PublishDay', 'TimeOfDay']\n",
    "    for feature in categorical_features:\n",
    "        ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        encoded = ohe.fit_transform(df[[feature]])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out([feature]))\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df['Sentiment_mean_scaled'] = scaler.fit_transform(df[['Sentiment_mean']].values)\n",
    "\n",
    "    df['is_weekend'] = (df['PublishDay_Saturday'] == 1) | (df['PublishDay_Sunday'] == 1)\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"  # Return empty string instead of list for CountVectorizer\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = [word for word in text.split() if word not in stop_words]\n",
    "        return \" \".join(tokens)  # Return space-separated string\n",
    "\n",
    "    df['CleanedHeadline'] = df['Headline'].apply(preprocess_text)\n",
    "    df['CleanedTitle'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "    headline_vectorizer = CountVectorizer(max_features=20, ngram_range=(5, 5))\n",
    "    headline_patterns = headline_vectorizer.fit_transform(df['CleanedHeadline']).toarray()\n",
    "    df = pd.concat([df, pd.DataFrame(headline_patterns, columns=headline_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "    title_vectorizer = CountVectorizer(max_features=20, ngram_range=(3, 3))\n",
    "    title_patterns = title_vectorizer.fit_transform(df['CleanedTitle']).toarray()\n",
    "    df = pd.concat([df, pd.DataFrame(title_patterns, columns=title_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "    feature_cols = list(df.filter(like='is_')) + list(df.filter(like='PublishDay_')) + list(df.filter(like='TimeOfDay_')) + ['Sentiment_mean_scaled'] + list(df.filter(like='source_is_')) + list(headline_vectorizer.get_feature_names_out()) + list(title_vectorizer.get_feature_names_out()) + ['is_weekend']\n",
    "    X = df[feature_cols]\n",
    "    X = X.loc[:, (X != X.iloc[0]).any()]\n",
    "\n",
    "    return X, df[['Facebook', 'GooglePlus', 'LinkedIn']]\n",
    "\n",
    "\n",
    "\n",
    "X, y = preprocess_data(df)\n",
    "\n",
    "targets = y.columns\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    y_target = y[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_target, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    for n_estimators in range(10, 101, 10):\n",
    "        rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train_scaled, y_train)\n",
    "        y_pred = rf_model.predict(X_test_scaled)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        if rmse < best_rmse:\n",
    "            best_model = rf_model\n",
    "            best_rmse = rmse\n",
    "            print(f\"Improved RMSE for {target}: {best_rmse:.4f} with n_estimators={n_estimators}\")\n",
    "\n",
    "    try:\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(np.expm1(y_test), np.expm1(y_pred)) #Inverse log transformation\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(np.expm1(y_test), np.expm1(y_pred))\n",
    "        results[target] = {'MSE': mse, 'RMSE': rmse, 'R-squared': r2}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction or evaluation for {target}: {e}\")\n",
    "\n",
    "for target, metrics in results.items():\n",
    "    print(f\"Results for {target} (Random Forest):\")\n",
    "    print(f\"  MSE: {metrics['MSE']:.2f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f}\")\n",
    "    print(f\"  R-squared: {metrics['R-squared']:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADHYATMA SHARMA\\anaconda3\\envs\\personaluse\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ADHYATMA SHARMA\\anaconda3\\envs\\personaluse\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ADHYATMA SHARMA\\anaconda3\\envs\\personaluse\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ADHYATMA SHARMA\\anaconda3\\envs\\personaluse\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ADHYATMA SHARMA\\anaconda3\\envs\\personaluse\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "583/583 [==============================] - 1s 1ms/step\n",
      "583/583 [==============================] - 1s 1ms/step\n",
      "583/583 [==============================] - 2s 2ms/step\n",
      "Results for Facebook (Neural Network):\n",
      "  MSE: 221747.57\n",
      "  RMSE: 470.90\n",
      "  R-squared: 0.38\n",
      "------------------------------\n",
      "Results for GooglePlus (Neural Network):\n",
      "  MSE: 184815.31\n",
      "  RMSE: 429.90\n",
      "  R-squared: 0.27\n",
      "------------------------------\n",
      "Results for LinkedIn (Neural Network):\n",
      "  MSE: 33008.16\n",
      "  RMSE: 181.68\n",
      "  R-squared: 0.38\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# One-Hot Encoding for Topic\n",
    "ohe_topic = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "topic_encoded = ohe_topic.fit_transform(df[['Topic']])\n",
    "topic_df = pd.DataFrame(topic_encoded, columns=ohe_topic.get_feature_names_out(['Topic']))\n",
    "df = pd.concat([df, topic_df], axis=1)\n",
    "\n",
    "# One-Hot Encoding for Day of the Week\n",
    "ohe_day = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "day_encoded = ohe_day.fit_transform(df[['PublishDay']])\n",
    "day_df = pd.DataFrame(day_encoded, columns=ohe_day.get_feature_names_out(['PublishDay']))\n",
    "df = pd.concat([df, day_df], axis=1)\n",
    "\n",
    "# One-Hot Encoding for Time of Day\n",
    "ohe_time = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "time_encoded = ohe_time.fit_transform(df[['TimeOfDay']])\n",
    "time_df = pd.DataFrame(time_encoded, columns=ohe_time.get_feature_names_out(['TimeOfDay']))\n",
    "df = pd.concat([df, time_df], axis=1)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = StandardScaler()\n",
    "df['Sentiment_mean_scaled'] = scaler.fit_transform(df[['Sentiment_mean']].values)\n",
    "\n",
    "# Feature Engineering: Source (Frequency Encoding)\n",
    "source_counts = df.groupby(['Topic', 'Source'])['Source'].count().unstack(fill_value=0)\n",
    "top_10_sources = {}\n",
    "for topic in df['Topic'].unique():\n",
    "    top_10_sources[topic] = source_counts.loc[topic].nlargest(10).index.tolist()\n",
    "\n",
    "for topic, sources in top_10_sources.items():\n",
    "    for source in sources:\n",
    "        source_col = f\"source_is_{topic}_{source.replace(' ', '_')}\"  # more robust column names\n",
    "        df[source_col] = (df['Topic'] == topic) & (df['Source'] == source)\n",
    "\n",
    "# Feature Engineering: Interaction Terms\n",
    "df['facebook_weekend'] = (df['Facebook'] > 0) & ((df['PublishDay_Saturday'] == 1) | (df['PublishDay_Sunday'] == 1))\n",
    "df['googleplus_weekend'] = (df['GooglePlus'] > 0) & ((df['PublishDay_Saturday'] == 1) | (df['PublishDay_Sunday'] == 1))\n",
    "df['linkedin_weekend'] = (df['LinkedIn'] > 0) & ((df['PublishDay_Saturday'] == 1) | (df['PublishDay_Sunday'] == 1))\n",
    "\n",
    "# Text Preprocessing (for Headline and Title)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float) and math.isnan(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df['CleanedHeadline'] = df['Headline'].apply(preprocess_text)\n",
    "df['CleanedTitle'] = df['Title'].apply(preprocess_text)\n",
    "\n",
    "# Feature Engineering: Common Words in Headline and Title (using CountVectorizer)\n",
    "headline_vectorizer = CountVectorizer(max_features=20, ngram_range=(5, 5))\n",
    "headline_patterns = headline_vectorizer.fit_transform(df['CleanedHeadline'].apply(lambda x: \" \".join(x))).toarray()\n",
    "df = pd.concat([df, pd.DataFrame(headline_patterns, columns=headline_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "title_vectorizer = CountVectorizer(max_features=20, ngram_range=(3, 3))\n",
    "title_patterns = title_vectorizer.fit_transform(df['CleanedTitle'].apply(lambda x: \" \".join(x))).toarray()\n",
    "df = pd.concat([df, pd.DataFrame(title_patterns, columns=title_vectorizer.get_feature_names_out())], axis=1)\n",
    "\n",
    "# ... (All Preprocessing and Feature Engineering steps - SAME AS IN THE PREVIOUS IMPROVED CODE)\n",
    "feature_cols = list(df.filter(like='is_')) + list(df.filter(like='PublishDay_')) + list(df.filter(like='TimeOfDay_')) + ['Sentiment_mean_scaled'] + list(df.filter(like='source_is_')) + list(df.filter(regex=r'\\b\\w+(?:\\s+\\w+){4}\\b')) + list(df.filter(regex=r'\\b\\w+(?:\\s+\\w+){2}\\b')) + ['facebook_weekend', 'googleplus_weekend', 'linkedin_weekend']\n",
    "\n",
    "X = df[feature_cols]\n",
    "X = X.loc[:, (X != X.iloc[0]).any()]\n",
    "\n",
    "targets = ['Facebook', 'GooglePlus', 'LinkedIn']\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Build the Neural Network Model\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=[X_train_scaled.shape[1]]),  # Input layer + hidden layer\n",
    "        layers.Dropout(0.3),  # Dropout for regularization\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1)  # Output layer (1 neuron for regression)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n",
    "\n",
    "    # Train the Model with Early Stopping\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,  # Adjust as needed\n",
    "        batch_size=32,  # Adjust as needed\n",
    "        validation_split=0.2, # Validation split\n",
    "        callbacks=[early_stopping],\n",
    "        verbose = 0\n",
    "    )\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred = y_pred.flatten()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results[target] = {'MSE': mse, 'RMSE': rmse, 'R-squared': r2}\n",
    "\n",
    "# Print results\n",
    "for target, metrics in results.items():\n",
    "    print(f\"Results for {target} (Neural Network):\")\n",
    "    print(f\"  MSE: {metrics['MSE']:.2f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.2f}\")\n",
    "    print(f\"  R-squared: {metrics['R-squared']:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personaluse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
